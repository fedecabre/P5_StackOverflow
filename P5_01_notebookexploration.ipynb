{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’objectif du notebook est de pré traiter les données issues de l’outil d’export de données de Stack Overflow-\n",
    "\"stackexchange explorer\", qui recense un grand nombre de données authentiques de la plateforme afin de détecter\n",
    "les sujets et générer des tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Étapes de prétraitement\n",
    "\n",
    "**1) Suppression du bruit**\n",
    "\n",
    "1. Suppression du formatage HTML\n",
    "2. Suppression des contractions\n",
    "3. La correction orthographique\n",
    "4. Mettre en minuscule le texte\n",
    "\n",
    "**2) Suppression des caractères simples**\n",
    "\n",
    "1. Suppression de la ponctuation, des caractères spéciaux et des nombres\n",
    "2. Suppression d'un seul caractère (facultatif et spécifique)\n",
    "\n",
    "**3) Suppression de StopWords**\n",
    "\n",
    "1. Suppression du mot le plus fréquent\n",
    "2. Suppression d'un certain type de mot (facultatif et spécifique)\n",
    "\n",
    "**4) Stemming / Lemmatisation**\n",
    "\n",
    "1. Stemming\n",
    "2. Lemmatisation\n",
    "\n",
    "\n",
    "Ce pré-processus est utilisé pour effectuer une simple détection de sujet (LDA, NMF, etc.) ou une classification,\n",
    "des informations nécessaires à certaines analyses peuvent être perdues.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.1 Vocabulaire\n",
    "Voici une petite liste de concepts utilisés dans ce cahier.\n",
    "\n",
    "Tokenize : \"Processus de conversion d'une chaîne en une liste de sous-chaînes, appelées tokens.\"\n",
    "\n",
    "Normalisation du texte : \"Processus de transformation d'un texte en une seule forme canonique qu'il n'aurait\n",
    "peut-être pas eu auparavant (par exemple, mettre en minuscule le texte, supprimer les contractions, correction\n",
    "orthographique, stemming / lemmatisation, etc.). La normalisation du texte nécessite de savoir quel type de texte\n",
    "doit être normalisée et comment elle doit être traitée par la suite ; il n’existe pas de procédure de\n",
    "normalisation universelle. \"\n",
    "\n",
    "Suppression du bruit : \"Processus de suppression de tout élément susceptible interferer avec votre analyse\n",
    "(par exemple, suppression du code HTML, mettre en minuscule le texte, suppression de la ponctuation / du caractère\n",
    "spécial, etc.)\n",
    "\n",
    "Stemming: \"Processus de réduction des mots à leur racine , base ou forme de racine - généralement une forme de\n",
    "mot écrit (\"fishing\", \"fished\", and \"fisher\" to the stem \"fish\").\"\n",
    "\n",
    "Lemmatisation : \"Processus de regroupement des formes fléchies d'un mot afin qu'elles puissent être analysées\n",
    "comme un seul élément, identifié par le lemme du mot, ou par la forme du dictionnaire (ie : \"walking\" to \"walk\",\n",
    " \"better\" to \"good\").\"\n",
    "\n",
    "StopWord : \"Mots qui sont filtrés avant ou après le traitement des données en langage naturel (texte). Les mots\n",
    "d'arrêt font généralement référence aux mots les plus courants dans une langue (des mots comme \"The\",\"a\", etc. en\n",
    "anglais).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Libraries and Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:50.566896Z",
     "start_time": "2021-06-17T07:22:44.165623Z"
    },
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/fedecabre/anaconda3/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/fedecabre/anaconda3/lib/python3.8/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/fedecabre/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: contractions in /home/fedecabre/anaconda3/lib/python3.8/site-packages (0.0.49)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/fedecabre/anaconda3/lib/python3.8/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in /home/fedecabre/anaconda3/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.2.0)\n",
      "Requirement already satisfied: pyahocorasick in /home/fedecabre/anaconda3/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Requirement already satisfied: autocorrect in /home/fedecabre/anaconda3/lib/python3.8/site-packages (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4\n",
    "! pip install contractions\n",
    "! pip install autocorrect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:51.465034Z",
     "start_time": "2021-06-17T07:22:50.568324Z"
    }
   },
   "outputs": [],
   "source": [
    "# generic librairies\n",
    "import pandas as pd\n",
    "\n",
    "# Text librairies\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:51.468162Z",
     "start_time": "2021-06-17T07:22:51.466135Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://numpy.org/devdocs/user/basics.types.html\n",
    "\n",
    "dtypes_questions = {'Id':'int32', 'Score': 'int16', 'Title': 'string', 'Body': 'string', 'Tags': 'string'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:51.693975Z",
     "start_time": "2021-06-17T07:22:51.469278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 200 ms, sys: 14.8 ms, total: 214 ms\n",
      "Wall time: 214 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nrows=10000\n",
    "\n",
    "df_questions_python = pd.read_csv('input/QueryResults python score more than 20.csv',\n",
    "                           usecols=dtypes_questions.keys(),\n",
    "                           encoding = \"ISO-8859-1\",\n",
    "                           dtype=dtypes_questions,\n",
    "                           nrows=nrows\n",
    "                          )\n",
    "\n",
    "df_questions_r = pd.read_csv('input/QueryResults r score more than 20.csv',\n",
    "                           usecols=['Id', 'Score', 'Title', 'Body', 'Tags'],\n",
    "                           encoding = \"ISO-8859-1\",\n",
    "                           dtype=dtypes_questions,\n",
    "                           nrows=nrows\n",
    "                          )\n",
    "\n",
    "df_questions = pd.concat([df_questions_python,df_questions_r])\n",
    "df_questions['Title_raw']=df_questions['Title']\n",
    "df_questions['Body_raw']=df_questions['Body']\n",
    "df_questions = df_questions.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:51.794897Z",
     "start_time": "2021-06-17T07:22:51.695193Z"
    }
   },
   "outputs": [],
   "source": [
    "df_questions[['Title', 'Body', 'Tags']] = df_questions[[\n",
    "    'Title', 'Body','Tags'\n",
    "]].applymap(lambda x: str(x).encode(\"utf-8\", errors='surrogatepass').decode(\n",
    "    \"ISO-8859-1\", errors='surrogatepass'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:51.806980Z",
     "start_time": "2021-06-17T07:22:51.796967Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id            int32\n",
       "Title        object\n",
       "Body         object\n",
       "Tags         object\n",
       "Score         int16\n",
       "Title_raw    string\n",
       "Body_raw     string\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:51.875362Z",
     "start_time": "2021-06-17T07:22:51.808951Z"
    }
   },
   "outputs": [],
   "source": [
    "spell = Speller()\n",
    "token = ToktokTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "charac = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789'\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "adjective_tag_list = {'JJ', 'JJR', 'JJS', 'RBR', 'RBS'}  # List of Adjective's tag from nltk package\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK Tag list**\n",
    "List of tag use in the tagger (pos_tag function) from NLTK:\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:51.892117Z",
     "start_time": "2021-06-17T07:22:51.876502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13913 entries, 0 to 13912\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Id         13913 non-null  int32 \n",
      " 1   Title      13913 non-null  object\n",
      " 2   Body       13913 non-null  object\n",
      " 3   Tags       13913 non-null  object\n",
      " 4   Score      13913 non-null  int16 \n",
      " 5   Title_raw  13913 non-null  string\n",
      " 6   Body_raw   13913 non-null  string\n",
      "dtypes: int16(1), int32(1), object(3), string(2)\n",
      "memory usage: 625.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_questions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:51.900445Z",
     "start_time": "2021-06-17T07:22:51.893133Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_questions['qTitle'] = df_questions['Tags'].apply(lambda x : len((x).split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Suppression du bruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La suppression du bruit consiste à supprimer tout ce qui peut interférer avec votre analyse de texte. C'est comme l'étape de nettoyage des données pour un projet ML classique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Suppression du code HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:51.910823Z",
     "start_time": "2021-06-17T07:22:51.901469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>I have a string containing an environment variable, e.g. </p>\\n\\n<pre><code>my_path = \\'$HOME/dir/dir2\\'\\n</code></pre>\\n\\n<p>I want parse the string, looking up the variable and replacing it in the string:</p>\\n\\n<pre><code>print \"HOME =\",os.environ[\\'HOME\\']\\nmy_expanded_path = parse_string(my_path)\\nprint \"PATH =\", my_expanded_path\\n</code></pre>\\n\\n<p>So I should see the output:</p>\\n\\n<pre><code>HOME = /home/user1\\n\\nPATH = /home/user1/dir/dir2\\n</code></pre>\\n\\n<p>Is there an elegant way to do that in Python?</p>\\n\\n<p>thanks!</p>\\n\\n<p>Conor</p>\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:58.584724Z",
     "start_time": "2021-06-17T07:22:51.911958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.62 s, sys: 47 ms, total: 6.66 s\n",
      "Wall time: 6.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Analyser la question et le titre puis renvoyer uniquement le texte\n",
    "df_questions['Body'] = df_questions['Body'].apply(\n",
    "    lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "df_questions['Title'] = df_questions['Title'].apply(\n",
    "    lambda x: BeautifulSoup(x, 'html.parser').get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup nous permet de supprimer efficacement la plupart du code html mais pas tout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:58.590401Z",
     "start_time": "2021-06-17T07:22:58.586504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have a string containing an environment variable, e.g. \\nmy_path = \\'$HOME/dir/dir2\\'\\n\\nI want parse the string, looking up the variable and replacing it in the string:\\nprint \"HOME =\",os.environ[\\'HOME\\']\\nmy_expanded_path = parse_string(my_path)\\nprint \"PATH =\", my_expanded_path\\n\\nSo I should see the output:\\nHOME = /home/user1\\n\\nPATH = /home/user1/dir/dir2\\n\\nIs there an elegant way to do that in Python?\\nthanks!\\nConor\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons donc supprimer le reste ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:58.600394Z",
     "start_time": "2021-06-17T07:22:58.592366Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" mettre la déf de la fonction\"\"\"\n",
    "    text = re.sub(r\"\\'\", \"'\", text) # match all literal apostrophe pattern then replace them by a single whitespace\n",
    "    text = re.sub(r\"\\n\", \" \", text) # match all literal Line Feed (New line) pattern then replace them by a single whitespace\n",
    "    text = re.sub(r\"\\xa0\", \" \", text) # match all literal non-breakable space pattern then replace them by a single whitespace\n",
    "    text = re.sub('\\s+', ' ', text) # match all one or more whitespace then replace them by a single whitespace\n",
    "    text = text.strip(' ')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:59.251308Z",
     "start_time": "2021-06-17T07:22:58.601845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 642 ms, sys: 0 ns, total: 642 ms\n",
      "Wall time: 641 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: clean_text(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:59.254966Z",
     "start_time": "2021-06-17T07:22:59.252406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have a string containing an environment variable, e.g. my_path = \\'$HOME/dir/dir2\\' I want parse the string, looking up the variable and replacing it in the string: print \"HOME =\",os.environ[\\'HOME\\'] my_expanded_path = parse_string(my_path) print \"PATH =\", my_expanded_path So I should see the output: HOME = /home/user1 PATH = /home/user1/dir/dir2 Is there an elegant way to do that in Python? thanks! Conor'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons aussi traiter la colonne des tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Suppression des contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:59.264925Z",
     "start_time": "2021-06-17T07:22:59.256024Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"développer les mots raccourcis, e.g. 'don't' to 'do not'\"\"\"\n",
    "    text = contractions.fix(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:59.943598Z",
     "start_time": "2021-06-17T07:22:59.266090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 669 ms, sys: 0 ns, total: 669 ms\n",
      "Wall time: 668 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: expand_contractions(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:59.947435Z",
     "start_time": "2021-06-17T07:22:59.944761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have a string containing an environment variable, e.g. my_path = \\'$HOME/dir/dir2\\' I want parse the string, looking up the variable and replacing it in the string: print \"HOME =\",os.environ[\\'HOME\\'] my_expanded_path = parse_string(my_path) print \"PATH =\", my_expanded_path So I should see the output: HOME = /home/user1 PATH = /home/user1/dir/dir2 Is there an elegant way to do that in Python? thanks! Conor'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. La correction orthographique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour 1000 entrées cette correction prends 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:59.958110Z",
     "start_time": "2021-06-17T07:22:59.948466Z"
    }
   },
   "outputs": [],
   "source": [
    "def autocorrect(text):\n",
    "    words_id = token.tokenize(text)\n",
    "    words_correct = [spell(i) for i in words_id]\n",
    "    return ' '.join(map(str, words_correct)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T11:27:19.840528Z",
     "start_time": "2021-05-27T11:27:19.832771Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_questions['Title'] = df_questions['Title'].apply(lambda x: autocorrect(x))\n",
    "#df_questions['Body'] = df_questions['Body'].apply(lambda x: autocorrect(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Mettre en minuscule le texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T11:27:19.851493Z",
     "start_time": "2021-05-27T11:27:19.841333Z"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Je choisis d'abaisser le texte après le paquet de contractions car celui-ci peut remettre des lettres majuscules lors de la suppression des contractions. La mise en minuscule du texte est une étape classique et utile de la suppression du bruit ou de la normalisation du texte car elle réduit le vocabulaire, normalise le texte et ne coûte presque rien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:59.990249Z",
     "start_time": "2021-06-17T07:22:59.959426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 ms, sys: 0 ns, total: 22.3 ms\n",
      "Wall time: 21.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_questions['Title'] = df_questions['Title'].str.lower()\n",
    "df_questions['Body'] = df_questions['Body'].str.lower()\n",
    "df_questions['Tags'] = df_questions['Tags'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:22:59.996089Z",
     "start_time": "2021-06-17T07:22:59.993256Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have a string containing an environment variable, e.g. my_path = \\'$home/dir/dir2\\' i want parse the string, looking up the variable and replacing it in the string: print \"home =\",os.environ[\\'home\\'] my_expanded_path = parse_string(my_path) print \"path =\", my_expanded_path so i should see the output: home = /home/user1 path = /home/user1/dir/dir2 is there an elegant way to do that in python? thanks! conor'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Suppression des caractères"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Suppression de la ponctuation, des caractères spéciaux et des nombres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T11:27:19.868801Z",
     "start_time": "2021-05-27T11:27:19.862864Z"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TOUS les caractères non alphabétiques ont été supprimés (y compris la ponctuation, les nombres et les caractères spéciaux). Ainsi, je ne considère pas les mots importants qui peuvent contenir des caractères spéciaux (comme \"C #\" en programmation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:00.004477Z",
     "start_time": "2021-06-17T07:22:59.998164Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation_and_number(text):\n",
    "    \"\"\"remove all punctuation and number\"\"\"\n",
    "    return text.translate(str.maketrans(\" \", \" \", charac)) \n",
    "\n",
    "def remove_non_alphabetical_character(text):\n",
    "    \"\"\"remove all non-alphabetical character\"\"\"\n",
    "    text = re.sub(\"[^a-z]+\", \" \", text) # remove all non-alphabetical character\n",
    "    text = re.sub(\"\\s+\", \" \", text) # remove whitespaces left after the last operation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:00.161514Z",
     "start_time": "2021-06-17T07:23:00.005555Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 147 ms, sys: 13 µs, total: 147 ms\n",
      "Wall time: 147 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: remove_punctuation_and_number(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: remove_punctuation_and_number(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:01.082891Z",
     "start_time": "2021-06-17T07:23:00.162945Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 917 ms, sys: 0 ns, total: 917 ms\n",
      "Wall time: 917 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: remove_non_alphabetical_character(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: remove_non_alphabetical_character(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:01.087790Z",
     "start_time": "2021-06-17T07:23:01.084226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have a string containing an environment variable eg mypath homedirdir i want parse the string looking up the variable and replacing it in the string print home osenvironhome myexpandedpath parsestringmypath print path myexpandedpath so i should see the output home homeuser path homeuserdirdir is there an elegant way to do that in python thanks conor'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Suppression de la présence d'un seul caractère\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je choisis de supprimer un seul caractère car lorsque nous faisons de la programmation, nous utilisons souvent un seul caractère alphabétique comme nom de variable (\"x\", \"y\", \"z\", etc.). Et j'ai observé que lorsque j'ai essayé de détecter des sujets sans les supprimer, j'ai trouvé beaucoup de sujets avec eux! Et même un sujet que je pourrais nommer \"Nom de variable\" ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:01.097499Z",
     "start_time": "2021-06-17T07:23:01.089320Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_single_letter(text):\n",
    "    \"\"\"remove single alphabetical character\"\"\"\n",
    "    text = re.sub(r\"\\b\\w\\b\", \"\", text) # remove all single letter\n",
    "    text = re.sub(\"\\s+\", \" \", text) # remove whitespaces left after the last operation\n",
    "    text = text.strip(\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:01.106807Z",
     "start_time": "2021-06-17T07:23:01.098921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#df_questions['Title'] = df_questions['Title'].apply(lambda x: remove_single_letter(x))\n",
    "#df_questions['Body'] = df_questions['Body'].apply(lambda x: remove_single_letter(x))\n",
    "\n",
    "# nous ne pouvons pas supprimer les single letters car nous voulons garder la lettre R !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:01.118188Z",
     "start_time": "2021-06-17T07:23:01.108067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have a string containing an environment variable eg mypath homedirdir i want parse the string looking up the variable and replacing it in the string print home osenvironhome myexpandedpath parsestringmypath print path myexpandedpath so i should see the output home homeuser path homeuserdirdir is there an elegant way to do that in python thanks conor'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Suppression des stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Removing most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supprimer les mots les plus fréquents est une étape classique de la NLP. Les mots les plus fréquents n'ajoutent pas beaucoup d'informations dans la plupart des cas (puisqu'ils sont dans presque toutes les phrases). En les supprimant, vous créez plus d'\"espace\" pour les autres mots qui peuvent avoir des informations plus utiles.\n",
    "Vous pouvez utiliser des listes prédéfinies à partir de bibliothèques telles que SciKit-Learn, NLTK et autres. Mais sachez que ces listes peuvent être plus problématiques qu'utiles (en particulier la liste scikit-learn, voir [Stop Word Lists in Free Open-source Software Packages](https://www.aclweb.org/anthology/W18-2502.pdf) pour plus d'informations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:01.126078Z",
     "start_time": "2021-06-17T07:23:01.119475Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"remove common words in english by using nltk.corpus's list\"\"\"\n",
    "    words_idx = token.tokenize(text)\n",
    "    filtered = [i for i in words_idx if not i in stop_words]\n",
    "    \n",
    "    return ' '.join(map(str, filtered)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:03.367188Z",
     "start_time": "2021-06-17T07:23:01.126979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.23 s, sys: 3.17 ms, total: 2.23 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: remove_stopwords(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:03.371907Z",
     "start_time": "2021-06-17T07:23:03.368650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string containing environment variable eg mypath homedirdir want parse string looking variable replacing string print home osenvironhome myexpandedpath parsestringmypath print path myexpandedpath see output home homeuser path homeuserdirdir elegant way python thanks conor'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Suppression d'adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je choisis de supprimer les adjectifs en plus de la liste NLTK. Pourquoi ? Tout simplement parce que lorsque j'ai d'abord essayé de faire une détection de sujet dans un cahier suivant celui-ci et cela améliore ma détection de sujet. Je pensais aussi que les adjectifs n'ajouteraient aucune information utile. En même temps, je pourrais aussi supprimer des verbes avec le même raisonnement. Mais je ne l'ai pas fait parce que l'ensemble de données StackOverflow concerne la programmation. Et en programmation, nous avons beaucoup de verbes, ou de mots qui peuvent être interprétés comme un verbe, qui peuvent être importants (\"return\", \"get\", \"request\", \"replace\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:03.380536Z",
     "start_time": "2021-06-17T07:23:03.373438Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# noinspection PyTypeChecker\n",
    "def remove_by_tag(text, undesired_tag):\n",
    "    \"\"\"remove all words by using ntk tag (adjectives, verbs, etc.)\"\"\"\n",
    "    words_idx = token.tokenize(text) # Tokenize each words\n",
    "    words_tagged = nltk.pos_tag(tokens=words_idx, tagset=None, lang='eng') # Tag each words and return a list of tuples (e.g. (\"have\", \"VB\"))\n",
    "    filtered = [i[0] for i in words_tagged if i[1] not in undesired_tag] # Select all words that don't have the undesired tags\n",
    "    \n",
    "    return ' '.join(map(str, filtered)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:46.470180Z",
     "start_time": "2021-06-17T07:23:03.381999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.9 s, sys: 134 ms, total: 43.1 s\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_questions['Title'] = df_questions['Title'].apply(\n",
    "    lambda x: remove_by_tag(x, adjective_tag_list))\n",
    "df_questions['Body'] = df_questions['Body'].apply(\n",
    "    lambda x: remove_by_tag(x, adjective_tag_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:46.474039Z",
     "start_time": "2021-06-17T07:23:46.471299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string containing environment mypath homedirdir want parse string looking replacing string print home osenvironhome myexpandedpath parsestringmypath print path myexpandedpath see output home homeuser path homeuserdirdir way thanks conor'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Stemming / Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Stemming et la Lemmatisation sont des opérations qui :\n",
    "- peuvent améliorer votre temps de calcul en réduisant votre vocabulaire\n",
    "- aider à généraliser plus facilement en regroupant les mots (ex: \"suis\", \"sont\", \"être\", etc. seront transformés en \"être\" pour la lemmatisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je n'ai pas choisi d'utiliser le stemming ici, mais l'on doit toujours envisager cette alternative car elle est beaucoup moins coûteuse.\n",
    "\n",
    "Le stemming est le processus de réduction des mots fléchis à leur racine mot, base ou forme de racine - généralement une forme de mot écrit (\"fishing\", \"fished\", and \"fisher\" à la racine \"fish\"). Il fonctionne généralement en supprimant l'affixe d'un mot. Un affixe peut être un suffixe ou un préfixe (par exemple «-ed», «-ing», etc.). C'est simple mais ne fonctionnera pas lorsque le mot est \"irrégulier\" (\"ran\" et \"run\"). C'est une opération plus simple que la lemmatisation, qui peut suffire dans certains cas, mais peut faire trop d'erreurs dans d'autres cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:46.485108Z",
     "start_time": "2021-06-17T07:23:46.474977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programer  :  program\n",
      "programing  :  program\n",
      "programers  :  program\n"
     ]
    }
   ],
   "source": [
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"]\n",
    "  \n",
    "for w in words:\n",
    "    print(w, \" : \", stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:46.494229Z",
     "start_time": "2021-06-17T07:23:46.486061Z"
    }
   },
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    \"\"\"Stem the text\"\"\"\n",
    "    words_idx = nltk.word_tokenize(text) # tokenize the text then return a list of tuple (token, nltk_tag)\n",
    "    stemmed_text = []\n",
    "    for word in words_idx:\n",
    "        stemmed_text.append(stemmer.stem(word)) # Stem each words\n",
    "    return \" \".join(stemmed_text) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:46.503282Z",
     "start_time": "2021-06-17T07:23:46.495299Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_questions['Title'] = df_questions['Title'].apply(lambda x: stem_text(x))\n",
    "# df_questions['Body'] = df_questions['Body'].apply(lambda x: stem_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Lemmatisation\n",
    "\n",
    "Comme dit au début, la lemmatisation est le processus de remplacement d'un mot par son lemma (forme canonique ou forme dictionnaire). Mais dans certains cas, un lemmatiseur peut ne pas être en mesure de trouver la bonne racine si vous ne précisez pas le type de mot comme vous pouvez le voir ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:47.515876Z",
     "start_time": "2021-06-17T07:23:46.504695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strip\n",
      "stripe\n",
      "are\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"stripes\", \"v\"))\n",
    "print(lemmatizer.lemmatize(\"stripes\", \"n\"))  \n",
    "print(lemmatizer.lemmatize(\"are\"))\n",
    "print(lemmatizer.lemmatize(\"are\", \"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une façon de contourner ce problème consiste à utiliser un marqueur et à passer le type de mot dans la fonction lemmatise. MAIS c'est vraiment coûteux. La mise en tige ou une simple lemmatisation à cet égard est bien plus efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:23:47.520271Z",
     "start_time": "2021-06-17T07:23:47.516974Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize the text by using tag \"\"\"\n",
    "    \n",
    "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))  # tokenize the text then return a list of tuple (token, nltk_tag)\n",
    "    lemmatise_text = []\n",
    "    for word, tag in tokens_tagged:\n",
    "        if tag.startswith('J'):\n",
    "            lemmatise_text.append(lemmatizer.lemmatize(word,'a')) # Lemmatise adjectives. Not doing anything since we remove all adjective\n",
    "        elif tag.startswith('V'):\n",
    "            lemmatise_text.append(lemmatizer.lemmatize(word,'v')) # Lemmatise verbs\n",
    "        elif tag.startswith('N'):\n",
    "            lemmatise_text.append(lemmatizer.lemmatize(word,'n')) # Lemmatise nouns\n",
    "        elif tag.startswith('R'):\n",
    "            lemmatise_text.append(lemmatizer.lemmatize(word,'r')) # Lemmatise adverbs\n",
    "        else:\n",
    "            lemmatise_text.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatisation\n",
    "    return \" \".join(lemmatise_text) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:24:35.260108Z",
     "start_time": "2021-06-17T07:23:47.521287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.6 s, sys: 87.6 ms, total: 47.7 s\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: lemmatize_text(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:24:35.264077Z",
     "start_time": "2021-06-17T07:24:35.261219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string contain environment mypath homedirdir want parse string look replace string print home osenvironhome myexpandedpath parsestringmypath print path myexpandedpath see output home homeuser path homeuserdirdir way thanks conor'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation du titre et du corps en même temps donne de bien meilleurs résultats pour la détection des sujets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:24:35.438478Z",
     "start_time": "2021-06-17T07:24:35.265153Z"
    }
   },
   "outputs": [],
   "source": [
    "df_questions['Text'] = df_questions['Title'] + ' ' + df_questions['Body']\n",
    "df_questions['Text_raw'] = df_questions['Title_raw'] + ' ' + df_questions['Body_raw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Dataframe avec les Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:24:35.445344Z",
     "start_time": "2021-06-17T07:24:35.439638Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<python><posix>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Tags'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:24:35.466824Z",
     "start_time": "2021-06-17T07:24:35.446560Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_split_tag(text):\n",
    "    \"\"\" \n",
    "        fonction pour traiter tous les tags en supprimant les caracteres HTML\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"<\", \"\", text) # match all literal apostrophe pattern then replace them by a single whitespace\n",
    "    text = re.sub(r\">\", \",\", text)# match all literal Line Feed (New line) pattern then replace them by a single whitespace\n",
    "    text = re.sub(r\" \", \"_\", text)\n",
    "    text = text.strip(',')\n",
    "    text = text.split(',')\n",
    "    #text = re.sub(r\",\", \" \", text)\n",
    "    #text = text.rstrip('>')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:24:35.608632Z",
     "start_time": "2021-06-17T07:24:35.467981Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 111 ms, sys: 8.04 ms, total: 119 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_questions['Tags'] = df_questions['Tags'].apply(lambda x: clean_split_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:24:35.643691Z",
     "start_time": "2021-06-17T07:24:35.610122Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Id               Tags\n",
      "0   3604587             python\n",
      "0   3604587  string-formatting\n",
      "1   1089662                 c#\n",
      "1   1089662             python\n",
      "1   1089662        compression\n",
      "1   1089662               zlib\n",
      "2   1185634             python\n",
      "2   1185634          algorithm\n",
      "3  15259547             python\n",
      "3  15259547                  r\n"
     ]
    }
   ],
   "source": [
    "tags_list = df_questions[['Id','Tags']].explode('Tags')\n",
    "print(tags_list.head(10))\n",
    "\n",
    "def untokenize(words_id):\n",
    "    return ' '.join(map(str, words_id)) # Return the text untokenize\n",
    "\n",
    "df_questions['Tags'] = df_questions['Tags'].apply(lambda x: untokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:24:35.651357Z",
     "start_time": "2021-06-17T07:24:35.644915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 python string-formatting\n",
       "1                               c# python compression zlib\n",
       "2                                         python algorithm\n",
       "3                               python r pandas data.table\n",
       "4                               python proxy urllib2 socks\n",
       "                               ...                        \n",
       "13908                                               python\n",
       "13909                  python pandas dataframe replace nan\n",
       "13910                             python django ironpython\n",
       "13911    python asynchronous twisted addition arithmeti...\n",
       "13912                                          python list\n",
       "Name: Tags, Length: 13913, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Exportation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T07:24:36.411085Z",
     "start_time": "2021-06-17T07:24:35.652599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Id                                     Title  \\\n",
      "0   3604587     use string format show zero precision   \n",
      "1   1089662     python inflate deflate implementation   \n",
      "2   1185634            solve mastermind guessing game   \n",
      "3  15259547                      sum pandas aggregate   \n",
      "4   2317849                     use sock proxy urllib   \n",
      "5  14899506  display message json object could decode   \n",
      "6  26405895     match fuzzy match string two datasets   \n",
      "7   5084743         print pretty string output python   \n",
      "8   5594933                    fabric cd manager work   \n",
      "9  28983292            apply family really vectorized   \n",
      "\n",
      "                                                Body  \\\n",
      "0  try represent number lead trail width include ...   \n",
      "1  interfacing server require data send deflate a...   \n",
      "2  would create solve follow puzzle mastermind ch...   \n",
      "3  recently make switch r python trouble get use ...   \n",
      "4                   use sock proxy download web page   \n",
      "5  python code load data long complicate json fil...   \n",
      "6  work way join two datasets base string name co...   \n",
      "7  list dicts field classid dept coursenum area t...   \n",
      "8  set development environment pc seem error cd c...   \n",
      "9  use say every r user apply vectorized check pa...   \n",
      "\n",
      "                                              Tags  Score  \\\n",
      "0                         python string-formatting     35   \n",
      "1                       c# python compression zlib     61   \n",
      "2                                 python algorithm     38   \n",
      "3                       python r pandas data.table     22   \n",
      "4                       python proxy urllib2 socks     48   \n",
      "5                                      python json    132   \n",
      "6  r string-matching fuzzy-search fuzzy-comparison     34   \n",
      "7              python string terminal pretty-print     29   \n",
      "8                                    python fabric     35   \n",
      "9          r performance loops vectorization apply    143   \n",
      "\n",
      "                                           Title_raw  \\\n",
      "0  How do I use string formatting to show BOTH le...   \n",
      "1        Python: Inflate and Deflate implementations   \n",
      "2       How to solve the \"Mastermind\" guessing game?   \n",
      "3              conditional sums for pandas aggregate   \n",
      "4      How can I use a SOCKS 4/5 proxy with urllib2?   \n",
      "5  Displaying better error message than \"No JSON ...   \n",
      "6  How can I match fuzzy match strings from two d...   \n",
      "7      How to Print \"Pretty\" String Output in Python   \n",
      "8          Fabric's cd context manager does not work   \n",
      "9      Is the \"*apply\" family really not vectorized?   \n",
      "\n",
      "                                            Body_raw  qTitle  \\\n",
      "0  <p>I'm trying to represent a number with leadi...       1   \n",
      "1  <p>I am interfacing with a server that require...       1   \n",
      "2  <p>How would you create an algorithm to solve ...       1   \n",
      "3  <p>I just recently made the switch from R to p...       1   \n",
      "4  <p>How can I use a SOCKS 4/5 proxy with urllib...       1   \n",
      "5  <p>Python code to load data from some long com...       1   \n",
      "6  <p>I've been working on a way to join two data...       1   \n",
      "7  <p>I have a list of dicts with the fields clas...       1   \n",
      "8  <p>I have set up my development environment on...       1   \n",
      "9  <p>So we are used to say to every R new user t...       1   \n",
      "\n",
      "                                                Text  \\\n",
      "0  use string format show zero precision try repr...   \n",
      "1  python inflate deflate implementation interfac...   \n",
      "2  solve mastermind guessing game would create so...   \n",
      "3  sum pandas aggregate recently make switch r py...   \n",
      "4  use sock proxy urllib use sock proxy download ...   \n",
      "5  display message json object could decode pytho...   \n",
      "6  match fuzzy match string two datasets work way...   \n",
      "7  print pretty string output python list dicts f...   \n",
      "8  fabric cd manager work set development environ...   \n",
      "9  apply family really vectorized use say every r...   \n",
      "\n",
      "                                            Text_raw  \n",
      "0  How do I use string formatting to show BOTH le...  \n",
      "1  Python: Inflate and Deflate implementations <p...  \n",
      "2  How to solve the \"Mastermind\" guessing game? <...  \n",
      "3  conditional sums for pandas aggregate <p>I jus...  \n",
      "4  How can I use a SOCKS 4/5 proxy with urllib2? ...  \n",
      "5  Displaying better error message than \"No JSON ...  \n",
      "6  How can I match fuzzy match strings from two d...  \n",
      "7  How to Print \"Pretty\" String Output in Python ...  \n",
      "8  Fabric's cd context manager does not work <p>I...  \n",
      "9  Is the \"*apply\" family really not vectorized? ...   \n",
      " \n",
      "          Id               Tags\n",
      "0   3604587             python\n",
      "0   3604587  string-formatting\n",
      "1   1089662                 c#\n",
      "1   1089662             python\n",
      "1   1089662        compression\n",
      "1   1089662               zlib\n",
      "2   1185634             python\n",
      "2   1185634          algorithm\n",
      "3  15259547             python\n",
      "3  15259547                  r\n"
     ]
    }
   ],
   "source": [
    "print(df_questions.head(10),'\\n \\n',tags_list.head(10))\n",
    "#df_questions.drop(['Tags'], axis=1).to_csv('df_questions_fullclean.csv', encoding='utf-8')\n",
    "df_questions.to_csv('df_questions_fullclean.csv', encoding='utf-8')\n",
    "tags_list.to_csv('tags_list.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}